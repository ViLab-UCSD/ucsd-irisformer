<!DOCTYPE html>
<html>
    <head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>IRISformer</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- <link rel="icon" type="image/png" href="./index_files/icon.png"> -->
    <link rel="stylesheet" href="./index_files/bootstrap.min.css">
    <link rel="stylesheet" href="./index_files/font-awesome.min.css">
    <link rel="stylesheet" href="./index_files/codemirror.min.css">
    <link rel="stylesheet" href="./index_files/app.css">
    <link rel="stylesheet" href="./index_files/bootstrap.min(1).css">

    <script type="text/javascript" async="" src="./index_files/analytics.js"></script>
    <script type="text/javascript" async="" src="./index_files/analytics(1).js"></script>
    <script async="" src="./index_files/js"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script>

    <script src="./index_files/jquery.min.js"></script>
    <script src="./index_files/bootstrap.min.js"></script>
    <script src="./index_files/codemirror.min.js"></script>
    <script src="./index_files/clipboard.min.js"></script>

    <script src="./index_files/app.js"></script>
</head>

<body data-gr-c-s-loaded="true">
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                <b>IRISformer</b>: Dense Vision Transformers <br>for Single-Image Inverse Rendering in Indoor Scenes
            <br />
            <small>
                CVPR 2022 (<b>Oral presentation</b>)
            </small>
            <br />
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://jerrypiglet.github.io/">
                          Rui Zhu
                        </a><sup>1</sup>
                    </li>
                    <li>
                        <a href="https://sites.google.com/a/eng.ucsd.edu/zhengqinli">
                          Zhengqin Li
                        </a><sup>1</sup>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=vfeeYI0AAAAJ&hl=en">
                          Janarbek Matai
                        </a><sup>2</sup>
                    </li>
                    <li>
                        <a href="http://www.porikli.com/">
                          Fatih Porikli
                        </a><sup>2</sup>
                    </li>
                    <li>
                        <a href="https://cseweb.ucsd.edu/~mkchandraker/">
                          Manmohan Chandraker
                        </a><sup>1</sup>
                    </li>
                </ul>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <sup>1</sup>UC San Diego
                    </li>
                    <li>
                        <sup>2</sup>Qualcomm AI Research
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2206.08423">
                            <img src="./index_files/paper.png" height="120px"><br>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.youtube.com/watch?v=Wy3P4LivzAc">
                            <img src="./index_files/youtube_icon_dark.png" height="120px"><br>
                                <h4><strong>Technical Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/ViLab-UCSD/IRISformer">
                            <img src="./index_files/github_pad.png" height="120px"><br>
                                <h4><strong>Source Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    &nbsp&nbsp&nbsp
                </h3>
                <!-- <video id="v0" width="100%" autoplay="" loop="" muted="" controls="">
                  <source src="https://www.youtube.com/watch?v=Wy3P4LivzAc" type="video/mp4">
                </video> -->
                <!-- <iframe width="100%" height="100%" src="https://www.youtube.com/embed/Wy3P4LivzAc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
                <div class="video-container">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/Wy3P4LivzAc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
            </div>
        </div>

        <br />
        <br />
        <br />
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <figure>
                    <img src="./index_files/teaser.png" class="img-responsive" alt="overview">
                    <figcaption>
                        <i>Given a single real world image, <b>IRISformer</b> simultaneously infers material (albedo and roughness), geometry (depth and normals), and spatially-varying lighting of the scene. The estimation enables virtual object insertion where we demonstrate high-quality photorealistic renderings in challenging lighting conditions compared to previous work (Li et al., 2021). The learned attention is also visualized for selected patches, indicating benefits of global attention to reason about distant interactions. </i>
                    </figcaption>
                </figure>
                <br>
                </iframe>
                <p class="text-justify">
                    Indoor scenes exhibit significant appearance variations due to myriad interactions between arbitrarily diverse object shapes, spatially-changing materials, and complex lighting. Shadows, highlights, and inter-reflections caused by visible and invisible light sources require reasoning about long-range interactions for  inverse rendering, which seeks to recover the components of image formation, namely, shape, material, and lighting. <br>
                    In this work, our intuition is that the long-range attention learned by transformer architectures is ideally suited to solve longstanding challenges in single-image inverse rendering. We demonstrate with a specific instantiation of a dense vision transformer, <b>IRISformer</b>, that excels at both single-task and multi-task reasoning required for inverse rendering. Specifically, we propose a transformer architecture to simultaneously estimate depths, normals, spatially-varying albedo, roughness and lighting from a single image of an indoor scene. Our extensive evaluations on benchmark datasets demonstrate state-of-the-art results on each of the above tasks, enabling applications like object insertion and material editing in a single unconstrained real image, with greater photorealism than prior works.
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Highlights
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            Given a single real-world image, inferring <b>material</b> (albedo and roughness), <b>geometry</b> (depth and normals), <b>spatially-varying lighting</b> of the scene. 
                        </li>
                        <li>
                            Two-stage model design with <b>Transformer-based</b> encoder-decoders:
                            <ol style="list-style-type: lower-alpha; padding-bottom: 0;">
                                <li style="margin-left:0em"><b>multi-task setting</b>: sharing encoder-decoders with smaller model;</li>
                                <li style="margin-left:0em; padding-bottom: 0;"><b>single-task setting</b>: independent encoder-decoders with better accuracy.</li>
                            </ol>
                        </li>
                        <li>
                            Demonstrating the benefits of <b>global attention</b> to reason about long-range interactions.
                        </li>
                        <li>
                            <b>State-of-the-art results</b> in:
                            <ol style="list-style-type: lower-alpha; padding-bottom: 0;">
                                <li style="margin-left:0em">per-pixel inverse rendering tasks on OpenRooms dataset;</li>
                                <li style="margin-left:0em">albedo estimation on IIW dataset;</li>
                                <li style="margin-left:0em; padding-bottom: 0;">object insertion on natural image datasets.</li>
                            </ol>
                        </li>
                    </ul>
                </p>
            </div>
        </div>
        
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Download
                </h3>
                <p class='text-justify'>To download the dataset, please send your request to the email <a>OpenRoomsDataset@gmail.com</a>. A download link will be sent to you once the dataset is released.
                </p>
            </div>
        </div> -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                We thank NSF CAREER 1751365, NSF IIS 2110409 and NSF CHASE-CI, generous support by Qualcomm, as well as gifts from Adobe and a Google Research Award. The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://vilab-ucsd.github.io/ucsd-openrooms/">OpenRooms (Zhengqin Li)</a>.
                <p></p>
            </div>
        </div>
    
    </div>


</body></html>
